{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOFdc5WpHG7Rn29Csusw4w6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-yZ4DaQ8kEl","executionInfo":{"status":"ok","timestamp":1714580266679,"user_tz":300,"elapsed":12994,"user":{"displayName":"Dakshyani Maddikunta","userId":"03652266451795984026"}},"outputId":"46f62a7e-c3a6-4da6-fee5-608e55986f83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["#Question1"]},{"cell_type":"code","source":[],"metadata":{"id":"1je30a-k_11-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RmjCmNxf_210"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import yfinance as yf\n","\n","# Download historical stock price data for AAPL from Yahoo Finance\n","data = yf.download('AAPL', start='2020-01-01', end='2022-01-01')\n","\n","# Save the downloaded data to a CSV file\n","data.to_csv('stock_prices.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CSysM4l8qva","executionInfo":{"status":"ok","timestamp":1714580885796,"user_tz":300,"elapsed":1774,"user":{"displayName":"Dakshyani Maddikunta","userId":"03652266451795984026"}},"outputId":"ac8eb952-e97f-4ed3-c8d1-66b8c2bfd343"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["\r[*********************100%%**********************]  1 of 1 completed\n"]}]},{"cell_type":"code","source":["print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBPqtmNN_DMq","executionInfo":{"status":"ok","timestamp":1714580930307,"user_tz":300,"elapsed":209,"user":{"displayName":"Dakshyani Maddikunta","userId":"03652266451795984026"}},"outputId":"dd303d87-c6ab-4f3d-c7fb-143bcd0ad4a4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["                  Open        High         Low       Close   Adj Close  \\\n","Date                                                                     \n","2020-01-02   74.059998   75.150002   73.797501   75.087502   73.059425   \n","2020-01-03   74.287498   75.144997   74.125000   74.357498   72.349129   \n","2020-01-06   73.447502   74.989998   73.187500   74.949997   72.925621   \n","2020-01-07   74.959999   75.224998   74.370003   74.597504   72.582657   \n","2020-01-08   74.290001   76.110001   74.290001   75.797501   73.750252   \n","...                ...         ...         ...         ...         ...   \n","2021-12-27  177.089996  180.419998  177.070007  180.330002  178.065659   \n","2021-12-28  180.160004  181.330002  178.529999  179.289993  177.038696   \n","2021-12-29  179.330002  180.630005  178.139999  179.380005  177.127609   \n","2021-12-30  179.470001  180.570007  178.089996  178.199997  175.962402   \n","2021-12-31  178.089996  179.229996  177.259995  177.570007  175.340317   \n","\n","               Volume  \n","Date                   \n","2020-01-02  135480400  \n","2020-01-03  146322800  \n","2020-01-06  118387200  \n","2020-01-07  108872000  \n","2020-01-08  132079200  \n","...               ...  \n","2021-12-27   74919600  \n","2021-12-28   79144300  \n","2021-12-29   62348900  \n","2021-12-30   59773000  \n","2021-12-31   64062300  \n","\n","[505 rows x 6 columns]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","\n","# Generate synthetic stock prices data\n","np.random.seed(0)  # for reproducibility\n","dates = pd.date_range(start='2023-01-01', periods=100, freq='D')\n","prices = np.random.uniform(50, 150, 100)  # random prices between 50 and 150\n","stock_data = pd.DataFrame({'Date': dates, 'Price': prices})\n","\n","# Print the first few rows of the dataset\n","print(\"Sample of the dataset:\")\n","print(stock_data.head())\n","\n","# Preprocess the dataset\n","\n","# Normalize the 'Price' column using Min-Max scaling\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","stock_data['Price'] = scaler.fit_transform(stock_data['Price'].values.reshape(-1, 1))\n","\n","# Split the dataset into training and test sets\n","train_size = int(len(stock_data) * 0.8)  # 80% for training, 20% for testing\n","train_data, test_data = stock_data[:train_size], stock_data[train_size:]\n","\n","# Optionally, you can reset the index of the splits\n","train_data.reset_index(drop=True, inplace=True)\n","test_data.reset_index(drop=True, inplace=True)\n","\n","# Optionally, you can save the processed datasets to files\n","train_data.to_csv('train_stock_data.csv', index=False)\n","test_data.to_csv('test_stock_data.csv', index=False)\n","\n","# Print the shapes of training and test sets\n","print(\"\\nTraining set shape:\", train_data.shape)\n","print(\"Test set shape:\", test_data.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"spJ3E39-_krN","executionInfo":{"status":"ok","timestamp":1714581056452,"user_tz":300,"elapsed":140,"user":{"displayName":"Dakshyani Maddikunta","userId":"03652266451795984026"}},"outputId":"93bfc750-9018-42aa-f30b-58150ed04ccc"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample of the dataset:\n","        Date       Price\n","0 2023-01-01  104.881350\n","1 2023-01-02  121.518937\n","2 2023-01-03  110.276338\n","3 2023-01-04  104.488318\n","4 2023-01-05   92.365480\n","\n","Training set shape: (80, 2)\n","Test set shape: (20, 2)\n"]}]},{"cell_type":"code","source":["#Question2"],"metadata":{"id":"SMbbwKHN_4cE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Dropout\n","\n","def create_lstm_model(input_shape, num_units=[50, 50], dropout_rate=0.2):\n","    \"\"\"\n","    Creates an LSTM-based architecture with multiple LSTM layers and dropout layers.\n","\n","    Args:\n","    - input_shape: Shape of the input sequences (e.g., (timesteps, features)).\n","    - num_units: List specifying the number of units/neurons in each LSTM layer.\n","    - dropout_rate: Dropout rate for the dropout layers.\n","\n","    Returns:\n","    - model: Compiled Keras model.\n","    \"\"\"\n","    model = Sequential()\n","\n","    # Add the first LSTM layer\n","    model.add(LSTM(units=num_units[0], return_sequences=True, input_shape=input_shape))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Add additional LSTM layers if specified\n","    for units in num_units[1:]:\n","        model.add(LSTM(units=units, return_sequences=True))\n","        model.add(Dropout(dropout_rate))\n","\n","    # Add the output\n"],"metadata":{"id":"gk7Tp906_6O7","executionInfo":{"status":"ok","timestamp":1714581193196,"user_tz":300,"elapsed":5674,"user":{"displayName":"Dakshyani Maddikunta","userId":"03652266451795984026"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["#Question3"],"metadata":{"id":"JRfNDxgMAOHU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n","\n","# Set parameters\n","max_features = 5000  # Number of words to consider as features\n","max_len = 200  # Maximum sequence length\n","embedding_size = 128\n","lstm_units = 64\n","dropout_rate = 0.2\n","num_lstm_layers = 2\n","epochs = 5\n","\n","# Load IMDB data\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# Pad sequences to ensure uniform input size\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen=max_len)\n","\n","# Build LSTM model\n","model = Sequential()\n","model.add(Embedding(max_features, embedding_size, input_length=max_len))\n","for _ in range(num_lstm_layers):\n","    model.add(LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\n","model.add(LSTM(lstm_units, dropout=dropout_rate, recurrent_dropout=dropout_rate))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(x_train, y_train, batch_size=128, epochs=epochs, validation_data=(x_test, y_test))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(x_test, y_test)\n","print(\"Test Loss:\", loss)\n","print(\"Test Accuracy:\", accuracy)\n","\n","# Plot training history\n","import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3-OSVQ_8AP0M","outputId":"33b7ea85-a18d-4007-b9aa-f0a01cb0856b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","Epoch 1/5\n","196/196 [==============================] - 391s 2s/step - loss: 0.1400 - accuracy: 0.7926 - val_loss: 0.0982 - val_accuracy: 0.8632\n","Epoch 2/5\n","187/196 [===========================>..] - ETA: 15s - loss: 0.0837 - accuracy: 0.8906"]}]},{"cell_type":"code","source":["#Question4"],"metadata":{"id":"A4aNQlGNB2iI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","# Assuming y_test contains the ground truth values and y_pred contains the model predictions\n","# y_test and y_pred should be numpy arrays or lists\n","\n","# Calculate evaluation metrics\n","mae = mean_absolute_error(y_test, y_pred)\n","rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","\n","print(f\"Mean Absolute Error: {mae}\")\n","print(f\"Root Mean Squared Error: {rmse}\")\n","\n","# Visualize predictions against ground truth\n","plt.figure(figsize=(8, 6))\n","plt.scatter(y_test, y_pred, color='blue', label='Predictions')\n","plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Ground Truth')\n","plt.xlabel('Ground Truth')\n","plt.ylabel('Predictions')\n","plt.title('Model Predictions vs Ground Truth')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"vrqtdP_oCVOx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#quesion5"],"metadata":{"id":"65vqQ_4wCWZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import uniform\n","\n","# Define hyperparameters distribution\n","param_dist = {\n","    'learning_rate': [0.001, 0.01, 0.1],\n","    'lstm_units': [50, 100, 150],\n","    'dropout_rate': uniform(loc=0.1, scale=0.4),  # Example for continuous parameter\n","}\n","\n","# Perform random search\n","random = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3)\n","random_result = random.fit(X_train, y_train)\n","\n","# Print results\n","print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))"],"metadata":{"id":"DDS9aF8YCYXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#question 6"],"metadata":{"id":"Onzm7Wf4Ci0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Challenges in LSTM Model Training and Optimization\n","Training LSTM models can be tricky due to several factors:\n","\n","Vanishing/Exploding Gradients: This is a common challenge in Recurrent Neural Networks (RNNs) of which LSTMs are a type. It can make learning long-term dependencies difficult.\n","Hyperparameter Tuning: LSTMs have many hyperparameters like the number of layers, units, and learning rate. Finding the optimal configuration requires experimentation.\n","Data Dependence: LSTM performance heavily relies on data quality and chosen features. Insufficient or noisy data can lead to poor forecasts.\n","Deciding on LSTM Layers and Units\n","The ideal number of layers and units depends on the complexity of the time series data. Here's a general approach:\n","\n","Start with a single LSTM layer with a moderate number of units (e.g., 32-64).\n","Train the model and evaluate its performance on a validation set.\n","Gradually increase the number of layers or units and re-evaluate.\n","Monitor for signs of overfitting and adjust hyperparameters accordingly.\n","Preprocessing Time Series Data for LSTM\n","Before feeding data into an LSTM, some preprocessing steps are crucial:\n","\n","Scaling: Normalize the data to a specific range (e.g., 0-1 or -1, 1) to prevent features with larger scales from dominating the learning process.\n","Feature Engineering: Create new features that might be helpful for forecasting, like rolling averages or seasonality indicators.\n","Lag creation: Convert the time series into a sequence of input-output pairs. Each input sequence contains past observations used to predict the next value (output).\n","Dropout Layers and Overfitting Prevention\n","Dropout layers randomly drop a percentage of neurons during training. This helps prevent the model from becoming overly reliant on specific features in the training data. By forcing the model to learn different representations with each training iteration, dropout reduces the risk of overfitting and improves generalization to unseen data.\n","\n","Analyzing Long-Term Dependency Capture and Prediction Accuracy\n","Evaluating an LSTM's ability to capture long-term dependencies can be done by comparing the model's forecasts on longer time horizons with actual values. Metrics like Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) can quantify prediction accuracy. Analyze the errors for patterns. Large errors far into the future might indicate the model struggles with long-term dependencies.\n","\n","Potential Improvements and Alternative Approaches\n","Here are some ways to enhance forecasting performance:\n","\n","Experiment with different hyperparameter configurations.\n","Try a stacked LSTM architecture with multiple LSTM layers.\n","Incorporate additional data sources that might be relevant for forecasting.\n","Explore alternative models like convolutional LSTMs (ConvLSTMs) or deep neural networks with LSTMs.\n","Remember, the best approach depends on the specific time series data and forecasting task."],"metadata":{"id":"7-X-iXXWCqwV"},"execution_count":null,"outputs":[]}]}